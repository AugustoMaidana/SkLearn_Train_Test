{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "div.warn{\n",
    "    display: all;\n",
    "    position: relative;\n",
    "    justify-content: center;\n",
    "    align-items: center;\n",
    "    color: black;\n",
    "    align: center;\n",
    "    font-family: Garamond;\n",
    "    font-size: 18px;\n",
    "    margin: 0em 0em 0em 0em;/* top, right, bottom, left */\n",
    "    width: 1000px;\n",
    "    height: 160px;\n",
    "    padding: 0.5em;\n",
    "    text-align: center;\n",
    "    text-indent: 0px;\n",
    "    text-shadow: 0px;\n",
    "    border-width:5px;\n",
    "    border-color:#ffffff;/*white;*/\n",
    "    border-style: double;\n",
    "    border-radius: 2px;\n",
    "    background-color: #00b8ff;\n",
    "}\n",
    "\n",
    "</style>\n",
    "<div class=warn>\n",
    "<img src=\"./data/maugustomaidana.jpg\" alt=\"Augusto Maidana\" align=\"left\" width=\"120px\" height=\"120px\" >\n",
    "<img src=\"./data/it_academy_logo.png\" alt=\"IT Academy\" align=\"bottom\" width=\"30%\" height=\"30%\" ><br>\n",
    "<h1><b>S11 T01:</b> Practicant amb training i test sets</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "<p style=\"text-align:left;\">\n",
    "    <b>M. Augusto Maidana Silanes</b>\n",
    "    <span style=\"float:right;\">\n",
    "        13/10/2021\n",
    "    </span>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Descripció\n",
    "\n",
    "Familiaritza't amb la programació científica mitjantçant la llibreria SKLearn / Scikitlearn.\n",
    "\n",
    "**Nivell 1**\n",
    "\n",
    "- Exercici 1:\n",
    "\n",
    "Parteix el conjunt de dades ``DelayedFlights.csv`` en train i test. Estudia els dos conjunts per separat, a nivell descriptiu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render our plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PowerTransformer, power_transform, quantile_transform, PolynomialFeatures\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.show(block=True)\n",
    "#pd.describe_option()\n",
    "plt.style.use('ggplot')\n",
    "#plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "\n",
    "directory = \"../estructures_dataframe/\"\n",
    "file = \"DelayedFlights.csv\"\n",
    "# Set floating-point format\n",
    "pd.set_option('float_format', '{:.2f}'.format)\n",
    "# Reading data from CSV: with CSV files a single line is needed to load the data:\n",
    "DelayedFlights_df = pd.read_csv(directory + file, index_col=False)\n",
    "DelayedFlights_df.reset_index(drop=True, inplace=True)\n",
    "DelayedFlights_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# let's see the DataFrame\n",
    "DelayedFlights_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going to print the number of rows and columns of the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Frame shape\n",
    "DelayedFlights_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following is a summary of some basic statistical details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a statistical description of the data set\n",
    "DelayedFlights_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the names of the columns of the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Frame columns\n",
    "DelayedFlights_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the \"UniqueCarrier\" column to \"AL\"\n",
    "DelayedFlights_df.rename({'UniqueCarrier':'AL'}, inplace=True,axis=1)\n",
    "DelayedFlights_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We select the columns we are going to work on and print the first 5 rows of the DataFrame\n",
    "\n",
    "#### Brief description of the columns and their types and units:\n",
    "\n",
    "**DepTime** :\n",
    "Actual Departure Time (stored as float, local time in hhmm)\n",
    "\n",
    "**ArrTime** :\n",
    "Actual Arrival Time (stored as float, local time in hhmm)\n",
    "\n",
    "**AirTime** :\n",
    "Airborne Time for the flight, in minutes (stored as integer).\n",
    "\n",
    "**ArrDelay** :\n",
    "Arrival Delay, in minutes (stored as integer).\n",
    "\n",
    "**DepDelay** :\n",
    "Departure Delay, in minutes (stored as integer).\n",
    "\n",
    "**Distance** :\n",
    "Flight Distance between airports (stored as integer, distance in miles = 1,60934 km)\n",
    "\n",
    "**CarrierDelay** :\n",
    "Delay, in minutes, attributable to the carrier (stored integer).\n",
    "\n",
    "**Note**: DepTime and ArrTime are the real times at which take-off and landing took place. The hours of the take-off and landing is coded as a float where the two first digits indicate the hour and the two last, the minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of the set of columns to work with\n",
    "subset_df = DelayedFlights_df[['AL', 'DepTime', 'ArrTime', 'AirTime', 'ArrDelay', 'DepDelay', 'Distance', 'CarrierDelay']].copy()\n",
    "subset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's print a concise summary of a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print non-null values\n",
    "subset_df.iloc[: 10000].info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print null values\n",
    "subset_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering to check for rows containing NANs in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating bool series True for NaN values\n",
    "mask = subset_df.loc[:,['ArrTime', 'AirTime', 'ArrDelay', 'CarrierDelay']].isna().any(axis=1)\n",
    "\n",
    "# filtering data: displaying data only with values = NaN \n",
    "subset_df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We eliminate the columns that have NAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping the rows whose all data is missing or contain null values (NaN)\n",
    "subset_df = subset_df.dropna(axis = 0, how ='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the NANs still exist in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of the numerical data\n",
    "subset_df.select_dtypes(exclude='object').hist(figsize=(20, 12), color='green', bins=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two flight frequency peaks are observed between 15-20 for takeoff and 20-24 hours for landing, DepTime and ArrTime respectively:\n",
    "\n",
    "**Note**: using bins = 24 we represent one bar per hour  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrate histogram details of Departure and Arrival Times\n",
    "features = ['DepTime', 'ArrTime']\n",
    "subset_df[features].hist(figsize=(14, 5), color='green', bins=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When calculating a kernel density estimate, a sawing effect is produced by the way the data is represented. By representing the hour with the first two digits and the minutes with the next two digits, we only have flights in the first 60 digits of the 100 with which the hour is represented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrate histogram details of Departure Times\n",
    "fig, ax = plt.subplots(figsize=(11, 9))\n",
    "sns.histplot(data=subset_df, x=\"DepTime\", stat='frequency', kde=True, common_norm=True, log_scale=False, element=\"step\", fill=True, bins=24, color='green')\n",
    "ax.set_xticks([0,400,800,1200,1600,2000,2400])\n",
    "ax.set_xticklabels(['00:00','04:00','08:00','12:00','16:00','20:00','00:00'])\n",
    "ax.set(xlabel='Departure Time (hh:mm)')\n",
    "ax.set_xlim([0,2400])\n",
    "ax.set_ylim([0,1250])\n",
    "ax.set_yticks([0,250,500,750,1000,1250])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have calculated the correlation between the variables and found very logical relationships, such as flight time being strongly related to distance or flight delay being related to departure delay. It is also observed that departure or arrival delays condition in the same way, almost 50% of airline delays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust plot\n",
    "#sns.set(rc={'figure.figsize': (14, 5)})\n",
    "output=\"corr_matrix.png\"\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_mat = subset_df.select_dtypes(include='number').corr()\n",
    "\n",
    "# Generate a mask for the upper triangle without the diagonal k = 0\n",
    "mask = np.triu(np.ones_like(corr_mat, dtype=bool), k=0)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "#cmap = sns.cubehelix_palette(20, light=0.95, dark=0.15)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns_plot = sns.heatmap(corr_mat, mask=mask,  annot=True, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "# save to file\n",
    "#fig = sns_plot.get_figure()\n",
    "#fig.savefig(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let’s split our data into features (X) and target (y) sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, let’s first split our data into training and testing sets:\n",
    "# Create X features (DataFrame)\n",
    "X = subset_df.loc[:,['AL', 'DepTime', 'ArrTime', 'AirTime', 'ArrDelay', 'DepDelay', 'Distance']]\n",
    "# Create y responses (Series)\n",
    "y = subset_df.loc[:,'CarrierDelay']\n",
    "#X = subset_df.iloc[:, :6]\n",
    "#y = subset_df.iloc[:, 6:7]\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a dataset into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null hypothesis test for training and test samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "alpha = 0.05\n",
    "null_test = {}\n",
    "for item in list(X_test.select_dtypes(exclude='object').columns):\n",
    "    stat, p = stats.ttest_ind(X_train[item], X_test[item], alternative= 'two-sided')\n",
    "    if p > alpha:\n",
    "\t    null_test[item] = [stat, p, alpha, \"Not-Ref\"]        \n",
    "    else:\n",
    "\t    null_test[item] = [stat, p, alpha, \"Refused\"]\n",
    "\n",
    "null_test = pd.DataFrame(null_test, index=['Stat', 'p', 'alpha', 'H0']).transpose()\n",
    "print('Null hypothesis test for Training and Test samples (H0 != H1):\\n', null_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nivell 2**\n",
    "\n",
    "- Exercici 2:\n",
    "\n",
    "Aplica algun procés de transformació (estandarditzar les dades numèriques, crear columnes dummies, polinomis...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain numerical data from the training data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop non-numeric cols on training data\n",
    "X_train_num = X_train._get_numeric_data() \n",
    "#X_train_num = X_train.select_dtypes(exclude='object')\n",
    "X_train_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain numerical data from the testing data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop non-numeric cols on test data\n",
    "X_test_num = X_train._get_numeric_data() \n",
    "X_test_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power transformations are applied to make the data more Gaussian by means of a family of parametric and monotonic transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num_power = power_transform(X_train_num, standardize=True)\n",
    "X_train_num_power = pd.DataFrame(X_train_num_power, columns=X_train_num.columns)\n",
    "X_train_num_power.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We apply a type of Quantile Transformation to transform the features to a normal distribution using the quantile information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num_quant = quantile_transform(X_train_num, output_distribution='normal', random_state=42)\n",
    "X_train_num_quant = pd.DataFrame(X_train_num_quant, columns=X_train_num.columns)\n",
    "X_train_num_quant.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We perform a second-degree polynomial decomposition of the AirTime and Distance features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_time_dist = X_train_num[['AirTime','Distance']]\n",
    "polynom = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_time_dist = pd.DataFrame(polynom.fit_transform(X_train_time_dist), columns=['AirTime(X1)', 'Distance(X2)', 'X1^2', 'X1·X2','X2^2'])\n",
    "X_train_time_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power transform the AirTime data\n",
    "power = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "AirTime = X_train_num.AirTime.values.reshape(-1, 1)\n",
    "data_trans = power.fit_transform(AirTime)\n",
    "AirTime_df = pd.DataFrame(data_trans, columns=['AirTime'])\n",
    "# histogram of the transformed data\n",
    "sns.displot(data=AirTime_df, legend=False, bins=50, color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve just the numeric input values\n",
    "data = X_train_num.values[:,:]\n",
    "# perform a box-cox transform of the dataset\n",
    "scaler = MinMaxScaler(feature_range=(1, 2))\n",
    "power = PowerTransformer(method='box-cox')\n",
    "pipeline = Pipeline(steps=[('s', scaler),('p', power)])\n",
    "data = pipeline.fit_transform(data)\n",
    "# convert the array back to a dataframe\n",
    "dataset = pd.DataFrame(data, columns = ['DepTime','ArrTime','AirTime','ArrDelay','DepDelay','Distance'])\n",
    "dataset.head()\n",
    "# histograms of the variables\n",
    "#dataset.hist(color='green', bins=50)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms of the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the columns to be plotted\n",
    "cols = ['DepTime', 'ArrTime','DepDelay','ArrDelay','AirTime','Distance']\n",
    "\n",
    "# create the figure and axes\n",
    "# sharex, sharey: bool or {'none', 'all', 'row', 'col'}, default: False\n",
    "fig, axes = plt.subplots(3, 2, sharex='none', sharey=True, squeeze=False, gridspec_kw={\"height_ratios\":(.85,.85,.85)}, figsize=(10,10))\n",
    "fig.suptitle('Power transformer: Box-Cox method')\n",
    "axes = axes.ravel()  # flattening the array makes indexing easier\n",
    "\n",
    "# stat: str\n",
    "# Aggregate statistic to compute in each bin.\n",
    "# count: show the number of observations in each bin\n",
    "# frequency: show the number of observations divided by the bin width\n",
    "# probability: or proportion: normalize such that bar heights sum to 1\n",
    "# percent: normalize such that bar heights sum to 100\n",
    "# density: normalize such that the total area of the histogram equals 1\n",
    "\n",
    "for col, ax in zip(cols, axes):\n",
    "    sns.histplot(data=dataset[col], color='green', kde=False, stat='count', ax=ax)\n",
    "    ax.set_title('Feature: {} '.format(col))\n",
    "    #ax.set(xlabel=col, ylabel='')\n",
    "    ax.set(xlim=(-3,3), ylim=(0, None))\n",
    "    x_axis = ax.axes.get_xaxis()\n",
    "    x_axis.set_label_text('foo')\n",
    "    x_label = x_axis.get_label()\n",
    "    x_label.set_visible(False)\n",
    "\n",
    "#plt.legend(title='', loc='upper left', labels=['Count'], bbox_to_anchor=(1, 1))\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization using sklearn\n",
    "\n",
    "## What is Normalization?\n",
    "\n",
    "**Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling**.\n",
    "\n",
    "Here’s the formula for normalization:\n",
    "\n",
    "$$\n",
    "X' = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "$$\n",
    "\n",
    "Here, $X_{max}$ and $X_{min}$ are the maximum and the minimum values of the feature respectively.\n",
    "\n",
    "* When the value of $X$ is the minimum value in the column, the numerator will be $0$, and hence $X'$ is $0$.\n",
    "* On the other hand, when the value of X is the maximum value in the column, the numerator is equal to the denominator and thus the value of $X'$ is $1$.\n",
    "* If the value of $X$ is between the minimum and the maximum value, then the value of $X'$ is between $0$ and $1$.\n",
    "\n",
    "Let's try to normalizate our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data normalization with sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# copy of datasets\n",
    "X_train_norm = X_train_num.copy()\n",
    "X_test_norm = X_test_num.copy()\n",
    "\n",
    "# fit scaler on training data\n",
    "norm = MinMaxScaler().fit(X_train_norm)\n",
    "\n",
    "# transform training data\n",
    "X_train_norm = norm.transform(X_train_norm)\n",
    "X_train_norm = pd.DataFrame(X_train_norm, columns=['DepTime', 'ArrTime', 'AirTime', 'ArrDelay', 'DepDelay','Distance'])\n",
    "print(\"Scaled Train Data: \\n\\n\")\n",
    "print(X_train_norm.head())\n",
    "\n",
    "# transform testing data\n",
    "X_test_norm = norm.transform(X_test_norm)\n",
    "X_test_norm = pd.DataFrame(X_test_norm, columns=['DepTime', 'ArrTime', 'AirTime', 'ArrDelay', 'DepDelay','Distance'])\n",
    "print(\"\\n\\nScaled Test Data: \\n\\n\")\n",
    "print(X_test_norm.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization using sklearn\n",
    "\n",
    "## What is Standardization?\n",
    "\n",
    "**Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation**.\n",
    "\n",
    "Here’s the formula for standardization:\n",
    "\n",
    "$$\n",
    "X' = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "* Feature scaling: $\\mu$ is the mean of the feature values and\n",
    "* Feature scaling: $\\sigma$ is the standard deviation of the feature values. Note that in this case, the values are not restricted to a particular range.\n",
    "\n",
    "Let’s try to standardize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data standardization with sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# copy of datasets\n",
    "X_train_stand = X_train_num.copy()\n",
    "X_test_stand = X_test_num.copy()\n",
    "\n",
    "# numerical features\n",
    "num_cols = ['DepTime', 'ArrTime', 'AirTime', 'ArrDelay', 'DepDelay','Distance']\n",
    "\n",
    "# apply standardization on numerical features\n",
    "for i in num_cols:\n",
    "\n",
    "    # fit on training data column\n",
    "    scale = StandardScaler().fit(X_train_stand[[i]])\n",
    "    \n",
    "    # transform the training data column\n",
    "    X_train_stand[i] = scale.transform(X_train_stand[[i]])\n",
    "    \n",
    "    # transform the testing data column\n",
    "    X_test_stand[i] = scale.transform(X_test_stand[[i]])\n",
    "\n",
    "X_train_stand.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams['figure.figsize'] = (8, 12)\n",
    "fig, axes = plt.subplots(3, 1)\n",
    "sns.boxplot(data = X_train, orient='v' , ax=axes[0]).set(title='Original Data')\n",
    "sns.boxplot(data = X_train_norm, orient='v' , ax=axes[1]).set(title='Normalized Data')\n",
    "sns.boxplot(data = X_train_stand, orient='v' , ax=axes[2]).set(title='Standardized Data')\n",
    "# move overall title up\n",
    "fig.subplots_adjust(hspace = 0.2)\n",
    "\n",
    "# add overall title\n",
    "fig.suptitle('Comparison Data', x=0.5, y=0.93, fontsize=14, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the labels in the categorical parameters\n",
    "print(\"There are {} Airlines\".format(len(X_train['AL'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the label counts in the categorical parameters\n",
    "print(\"Each airline has the following number of entries :\")\n",
    "X_train['AL'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Each airline has the following number of entries in percentage (%):\")\n",
    "X_train.AL.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = X_train.AL.value_counts(normalize=True)[:15].sum().round(2)\n",
    "print(\"The 15 airlines with the most registrations represent {} % of the total.:\".format(p*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The list of the top 15 airlines is as follows:\")\n",
    "top_15 = list(X_train.AL.value_counts(normalize=True)[:15].index)\n",
    "print(*top_15, sep = \", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a mask for filtering airlines\n",
    "mask = X_train['AL'].isin(top_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new subset of data\n",
    "subset_AL = X_train.loc[:, ['AL']]\n",
    "subset_AL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We created a new group with airlines that do not belong to the group of 15 airlines\n",
    "subset_AL.loc[~mask,'AL'] = 'Others'\n",
    "subset_AL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a copy of the data frame\n",
    "set_df = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We label a new airline group \"Other LA\" with all airlines other than the 15 listed.\n",
    "set_df.loc[~mask,'AL'] = 'Others'\n",
    "set_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we group the records by airlines\n",
    "grp = set_df.groupby(['AL']).sum()\n",
    "grp.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we grouped the departure delay records by airlines and calculated their average value and the number of records\n",
    "set2_df = set_df[['AL','ArrDelay']].groupby('AL').agg(['mean','count'])\\\n",
    "    .sort_values(by=('ArrDelay','mean'), ascending=False).round(2)\n",
    "set2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we remove the multiple index and rename the columns\n",
    "set2_df.columns =[\"Mean\",\"Count\"]\n",
    "set2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we restart the index\n",
    "set2_df = set2_df.reset_index()\n",
    "set2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as pltc\n",
    "from random import sample\n",
    "# Defining colors\n",
    "all_colors = [k for k,v in pltc.cnames.items()]\n",
    "colors = sample(all_colors, 16)#len(global_stats)\n",
    "print('colors: ', colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "cars = set2_df.AL.values\n",
    "data = set2_df.Mean.values\n",
    "\n",
    "# Creating explode data\n",
    "explode = (0.1, 0.0, 0.2, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "#np.set_printoptions(precision = 1, suppress = True)\n",
    "#explode = np.random.uniform(0,0.3,16)\n",
    "#explode = tuple(explode)\n",
    "\n",
    "# Creating color parameters\n",
    "colors = tuple(colors)\n",
    " \n",
    "# Wedge properties\n",
    "wp = {'linewidth': 1, 'edgecolor': \"green\"}\n",
    " \n",
    "# Creating autopct arguments\n",
    "def func(pct, allvalues):\n",
    "    absolute = int(pct / 100.*np.sum(allvalues))\n",
    "    return \"{:.1f}%\\n({:d} g)\".format(pct, absolute)\n",
    "\n",
    "# Creating plot\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "wedges, texts, autotexts = ax.pie(data, autopct=lambda pct: func(pct, data), explode=explode, labels=cars, \\\n",
    "    shadow=True, colors=colors, startangle=90, wedgeprops=wp, textprops=dict(color=\"black\"))\n",
    " \n",
    "# Adding legend\n",
    "ax.legend(wedges, set2_df.AL, title=\"Airlines\", loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "plt.setp(autotexts, size=8, weight=\"bold\")\n",
    "ax.set_title(\"Arrival Delay\")\n",
    " \n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  plot the mean of the records by airline\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(x = \"AL\", y = \"Mean\", data = set2_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the number of records by airline \n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(x = \"AL\", y = \"Count\", data = set2_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping the country column \n",
    "set_df = set_df.drop(['DepTime','ArrTime','AirTime','ArrDelay','DepDelay'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = pd.get_dummies(set_df['AL'])\n",
    "print(round(encoded_data, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nivell 3**\n",
    "\n",
    "- Exercici 3:\n",
    "  \n",
    "Resumeix les noves columnes generades de manera estadística i gràfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges of One-Hot Encoding: Dummy Variable Trap\n",
    "\n",
    "One-Hot Encoding results in a Dummy Variable Trap as the outcome of one variable can easily be predicted with the help of the remaining variables.\n",
    "\n",
    "> **Dummy Variable Trap is a scenario in which variables are highly correlated to each other.**\n",
    "\n",
    "The Dummy Variable Trap leads to the problem known as **multicollinearity**. Multicollinearity occurs where there is a dependency between the independent features. Multicollinearity is a serious issue in machine learning models like [Linear Regression](https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/?utm_source=blog&utm_medium=one-hot-encoding-vs-label-encoding-using-scikit-learn) and [Logistic Regression](https://www.analyticsvidhya.com/blog/2015/11/beginners-guide-on-logistic-regression-in-r/?utm_source=blog&utm_medium=one-hot-encoding-vs-label-encoding-using-scikit-learn).\n",
    "\n",
    "So, in order to overcome the problem of multicollinearity, one of the dummy variables has to be dropped. Here, I will practically demonstrate how the problem of multicollinearity is introduced after carrying out the one-hot encoding.\n",
    "\n",
    "One of the common ways to check for multicollinearity is the Variance Inflation Factor (VIF):\n",
    "\n",
    "* VIF=1, Very Less Multicollinearity\n",
    "* VIF<5, Moderate Multicollinearity\n",
    "* VIF>5, Extreme Multicollinearity (This is what we have to avoid)\n",
    "\n",
    "Compute the VIF scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "# Function to calculate VIF\n",
    "def calculate_vif(data):\n",
    "    vif_df = pd.DataFrame(columns = ['Var', 'Vif'])\n",
    "    x_var_names = data.columns\n",
    "    for i in range(0, x_var_names.shape[0]):\n",
    "        y = data[x_var_names[i]]\n",
    "        x = data[x_var_names.drop([x_var_names[i]])]\n",
    "        r_squared = sm.OLS(y,x).fit().rsquared\n",
    "        vif = round(1/(1-r_squared),2)\n",
    "        vif_df.loc[i] = [x_var_names[i], round(vif, 1)]\n",
    "    return vif_df.sort_values(by = 'Vif', axis = 0, ascending=False, inplace=False)\n",
    "\n",
    "df = encoded_data.copy()\n",
    "calculate_vif(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = encoded_data.copy()\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "#print(round(df2, 1))\n",
    "# For each X, calculate VIF and save in dataframe\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(df2.values, i) for i in range(df2.shape[1])]\n",
    "vif[\"features\"] = df2.columns\n",
    "vif.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Inflation Factor (VIF) Explained\n",
    "\n",
    "Colinearity is the state where two variables are highly correlated and contain similiar information about the variance within a given dataset. To detect colinearity among variables, simply create a correlation matrix and find variables with large absolute values. In R use the corr function and in python this can by accomplished by using numpy's corrcoef function.\n",
    "\n",
    "Multicolinearity on the other hand is more troublesome to detect because it emerges when three or more variables, which are highly correlated, are included within a model. To make matters worst multicolinearity can emerge even when isolated pairs of variables are not colinear.\n",
    "\n",
    "A common R function used for testing regression assumptions and specifically multicolinearity is ``\"VIF()\"`` and unlike many statistical concepts, its formula is straightforward:\n",
    "\n",
    "$$\n",
    "\\text{V.I.F.} = \\frac{1}{1-R^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num.hist(figsize=(20, 12), color='#32a850', bins=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The applied power transformation has transformed the features generating a standardized distribution of mean = 0 and std = 1 with different minimum and maximum values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num_power.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num_power.hist(figsize=(20, 12), color='#32a850', bins=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The application of a quantile-type transformation with respect to the original distribution makes it look more like a Gaussian distribution. In cases where a bias has been detected, if it has not cancelled it out in the worst case it is possible to see a decrease in the bias.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num_quant.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num_quant.hist(figsize=(20, 12), color='#32a850', bins=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_time_dist.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_time_dist.hist(figsize=(20, 12), color='#32a850', bins=24)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e2cdb4300706d64c3bb8ce1a3dbea36fdccde4f6e84f712979922e3f9215c9d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('myhpc': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
